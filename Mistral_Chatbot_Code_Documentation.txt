# Mistral Chatbot – Code Walkthrough

This document explains how the chatbot code works **line by line**.

---

## 1. Imports and Setup

```python
import os
from mistralai import Mistral
# import tiktoken
from dotenv import load_dotenv
```

* `import os`
  Used to access environment variables (like your API key) via `os.getenv()`.

* `from mistralai import Mistral`
  Imports the `Mistral` client class from the Mistral Python SDK. This lets your code talk to the Mistral API.

* `# import tiktoken`
  Commented out. You don’t use `tiktoken` yet, so it’s just ignored.

* `from dotenv import load_dotenv`
  Imports `load_dotenv`, which loads variables from a `.env` file into environment variables (e.g., `API_KEY`).

---

## 2. Loading Environment Variables

```python
load_dotenv()
```

* This reads the `.env` file in your project folder.
* Any variables defined there (like `API_KEY=...`) become available through `os.getenv("API_KEY")`.
* This keeps your secret keys **out of the code** and safe from being pushed to GitHub.

Your `.env` should look like:

```text
API_KEY=your_mistral_api_key_here
```

---

## 3. Creating the Mistral Client & Configuration

```python
client = Mistral(api_key=os.getenv("API_KEY"))
MODEL="mistral-small-latest"
TEMPERATURE= 0.7
MAX_TOKENS = 100
SYSTEM_PROMPT = "You are an elegant and calm assistant who loves answering questions"
messages= [{"role": "system", "content": SYSTEM_PROMPT}]
```

* `client = Mistral(api_key=os.getenv("API_KEY"))`

  * `os.getenv("API_KEY")` fetches your API key from the environment.
  * `Mistral(...)` creates a client that will be used to send requests to the Mistral API.

* `MODEL = "mistral-small-latest"`
  The model name you want to use. This tells the API *which* AI model to call.

* `TEMPERATURE = 0.7`
  Controls **creativity** of the responses:

  * Lower (e.g., 0.1) = more deterministic, serious.
  * Higher (e.g., 0.9) = more creative, varied.

* `MAX_TOKENS = 100`
  The maximum number of tokens (roughly words/sub-words) the model is allowed to generate in its reply.

* `SYSTEM_PROMPT = "You are an elegant and calm assistant who loves answering questions"`
  This is the **personality / behavior instruction** for the assistant. It tells the model how to behave throughout the conversation.

* `messages = [{"role": "system", "content": SYSTEM_PROMPT}]`
  This list stores the **entire conversation history**.

  * The first message is a `system` message defining how the AI should respond.
  * Later, `user` and `assistant` messages are appended to this list.

---

## 4. The `chatbot` Function

```python
def chatbot(user_input):
    
    messages.append({"role": "user", "content": user_input})
    response = client.chat.complete(
        model= MODEL,
        messages=messages,
        temperature= TEMPERATURE, #temperature is like a scale for AI's creativity for the response
        max_tokens= MAX_TOKENS
    )

    reply = response.choices[0].message.content
    messages.append({"role": "assistant", "content": reply}) 
    return reply
```

This function takes the user’s text and returns the assistant’s reply.

### Step-by-step:

1. **Add the user message to history**

   ```python
   messages.append({"role": "user", "content": user_input})
   ```

   * Appends a new dictionary with:

     * `"role": "user"` – who is speaking.
     * `"content": user_input` – what the user just typed.
   * The `messages` list now includes the system prompt + all previous user/assistant messages + this new one.

2. **Send request to Mistral API**

   ```python
   response = client.chat.complete(
       model= MODEL,
       messages=messages,
       temperature= TEMPERATURE,
       max_tokens= MAX_TOKENS
   )
   ```

   * `client.chat.complete(...)` calls the chat completion endpoint.

   * Parameters:

     * `model` – which model to use.
     * `messages` – entire conversation so far (context).
     * `temperature` – creativity level.
     * `max_tokens` – limit on reply length.

   * `response` now contains the model’s answer and some metadata.

3. **Extract assistant’s reply**

   ```python
   reply = response.choices[0].message.content
   ```

   * The response can contain multiple choices; you take the **first** one.
   * `reply` becomes the text generated by the assistant.

4. **Add assistant reply to history**

   ```python
   messages.append({"role": "assistant", "content": reply})
   ```

   * Stores the assistant’s reply so future responses can see previous context.

5. **Return the reply**

   ```python
   return reply
   ```

   * This value is used later to print the answer in the terminal.

---

## 5. Main Chat Loop

```python
while True:
    user_input= input("You:")
    if user_input.strip().lower() in {"exit","quit"}:
        break
    answer = chatbot(user_input)
    
    print("Assitant:",answer)
```

This block runs the chatbot in the terminal.

### Step-by-step:

1. **Infinite loop**

   ```python
   while True:
   ```

   * Keeps asking the user for input until they decide to exit.

2. **Read user input**

   ```python
   user_input = input("You:")
   ```

   * Shows `You:` in the terminal.
   * Waits for the user to type a message and press Enter.

3. **Check for exit command**

   ```python
   if user_input.strip().lower() in {"exit","quit"}:
       break
   ```

   * `user_input.strip()` removes extra spaces.
   * `.lower()` makes it case-insensitive (`Exit`, `QUIT`, etc.).
   * If the user types `exit` or `quit`, the `while` loop is broken → program ends.

4. **Call the chatbot function**

   ```python
   answer = chatbot(user_input)
   ```

   * Sends the user’s message to the `chatbot()` function.
   * Receives the assistant’s reply in `answer`.

5. **Print the assistant’s reply**

   ```python
   print("Assistant:",answer)
   ```

   * Shows the reply in the terminal.
